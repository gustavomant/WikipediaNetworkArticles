{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4640c2-69ca-4c89-aadf-e4853d8e5f86",
   "metadata": {},
   "source": [
    "## Extração dos dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88d5246-fe32-476d-963b-c827649170e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 182 ms, sys: 37.1 ms, total: 219 ms\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install -q requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd6fbe0-399f-47bd-92d1-716485c29a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "def create_multistream_dir() -> None:\n",
    "    dirs = ['./multistream/compressed', './multistream/decompressed']\n",
    "\n",
    "    for path in dirs:\n",
    "        # Create dir recursively\n",
    "        # https://docs.python.org/3/library/os.html#os.makedirs\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_articles_details(dump_url: str) -> dict:\n",
    "    print('[INFO] Fetching dump info')\n",
    "\n",
    "    # https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request\n",
    "    resp = requests.get(url=dump_url)\n",
    "    # https://requests.readthedocs.io/en/latest/user/quickstart/#json-response-content\n",
    "    data = resp.json()\n",
    "\n",
    "    # (ex) See https://dumps.wikimedia.org/ptwiki/20240720/dumpstatus.json\n",
    "    articles_details = data['jobs']['articlesmultistreamdumprecombine']\n",
    "\n",
    "    # Check if 'dumps.wikimedia.org' (API) file's status is set to 'done'\n",
    "    check_articles_status(articles_details)\n",
    "\n",
    "    return articles_details\n",
    "\n",
    "\n",
    "def check_articles_status(articles_details: dict) -> None:\n",
    "    # Check if 'dumps.wikimedia.org' (API) file's status is set to 'done'\n",
    "    if articles_details['status'] != 'done':\n",
    "        raise Exception('\\'Article Multistream Dump Precombine\\' is not \\'done\\' (' + articles_details['status'] + ')')\n",
    "\n",
    "\n",
    "def get_file(dump_url: str) -> Tuple[dict, str]:\n",
    "    # Retrieve information about article dump from 'dumps.wikimedia.org' (API)\n",
    "    articles_details = get_articles_details(dump_url)\n",
    "\n",
    "    # Find file from dumps.wikimedia.org from 'articlesmultistreamdumprecombine' that endings with '-pages-articles-multistream.xml.bz2'\n",
    "    # Because there is also a '-pages-articles-multistream-index.txt.bz2' file included\n",
    "    # (ex) See https://dumps.wikimedia.org/ptwiki/20240720/dumpstatus.json\n",
    "    key = [file for file in articles_details['files'].keys() if file.endswith('-pages-articles-multistream.xml.bz2')][0]\n",
    "    file = articles_details['files'][key]\n",
    "    \n",
    "    return file, get_filename(file)\n",
    "\n",
    "\n",
    "def get_filename(file: dict) -> str:\n",
    "    # (ex) /ptwiki/20240720/ptwiki-20240720-pages-articles-multistream.xml.bz2 -> \n",
    "    #   -> ptwiki-20240720-pages-articles-multistream.xml.bz2\n",
    "    return file['url'].split('/')[-1]\n",
    "\n",
    "\n",
    "def check_sha1(path: str, filename: str) -> bool:\n",
    "    print('[INFO] Checking SHA1 Checksum of \\'' + filename + '\\'')\n",
    "\n",
    "    # https://docs.python.org/3/library/hashlib.html#hashlib.sha1\n",
    "    sha1 = hashlib.sha1()\n",
    "\n",
    "    # https://stackoverflow.com/a/22058673\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            data = f.read(65536) # BUFF_SIZE (arbitrary value)\n",
    "            if not data:\n",
    "                break\n",
    "            sha1.update(data)\n",
    "\n",
    "    # Returns SHA1 in hex format\n",
    "    # https://docs.python.org/3/library/hashlib.html#hashlib.hash.hexdigest\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "\n",
    "def check_disk_space(file: dict) -> None:\n",
    "    # Check free space in disk to download .bz2 archive\n",
    "    _, _, free = shutil.disk_usage(\"/\")\n",
    "    \n",
    "    if free < file['size']:\n",
    "        raise Exception(\"Not enough disk space (Needed \" + file.size + \" / \"+ free +\")\")\n",
    "\n",
    "\n",
    "def download_multistream(file: dict, filename: str, path: str) -> None:\n",
    "    # Check free space in disk to download .bz2 archive\n",
    "    check_disk_space(file)\n",
    "\n",
    "    # (ex) 'https://dumps.wikimedia.org' + '/ptwiki/20240720/ptwiki-20240720-pages-articles-multistream.xml.bz2'\n",
    "    url = 'https://dumps.wikimedia.org' + file['url']\n",
    "\n",
    "    # Download file from dumps.wikimedia.org\n",
    "    with requests.get(url, stream=True) as stream:\n",
    "        # https://3.python-requests.org/api/#requests.Response.raise_for_status\n",
    "        stream.raise_for_status()\n",
    "        # https://stackoverflow.com/a/44299915\n",
    "        total_size = int(stream.headers.get('content-length', 0))\n",
    "\n",
    "        # Progress bar\n",
    "        # https://tqdm.github.io/docs/tqdm/\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc='Downloading \\'' + filename + '\\'', initial=0, file=sys.stdout) as pbar:\n",
    "            with open(path, mode=\"wb\") as multistream_file:\n",
    "                for chunk in stream.iter_content(chunk_size= 10 * 1024):\n",
    "                    multistream_file.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "    # Compare downloaded file's SHA1 to expected SHA1 from dumps.wikimedia.org (API)\n",
    "    sha1_hex = check_sha1(path, filename)\n",
    "    if sha1_hex != file['sha1']:\n",
    "        raise Exception(\"SHA1 Checksum did not match (\" + sha1_hex + \"!=\" + file['sha1'] + \")\")\n",
    "\n",
    "    print('[INFO] Download completed')\n",
    "\n",
    "\n",
    "def get_multistream_file(wikinamedate: str) -> str:\n",
    "    # (ex) 'https://dumps.wikimedia.org/' + 'ptwiki/20240720' + '/dumpstatus.json'\n",
    "    dump_url = 'https://dumps.wikimedia.org/' + wikinamedate + '/dumpstatus.json'\n",
    "\n",
    "    # Get file JSON object from dumps.wikimedia.org API and its filename\n",
    "    file, filename = get_file(dump_url)\n",
    "    \n",
    "    # Check if file was already downloaded and decompressed\n",
    "    if os.path.isfile(os.path.join('./multistream/decompressed', filename[:-4])):\n",
    "        print('[INFO] Valid matching multistream found in multistream folder')\n",
    "    else:\n",
    "        # Check if file was already downloaded (waiting to be decompressed)\n",
    "        if os.path.isfile(os.path.join('./multistream/compressed', filename)):\n",
    "            # Check if downloaded file isn't corrupted\n",
    "            if check_sha1(os.path.join('./multistream/compressed', filename), filename) != file['sha1']:\n",
    "                print('[INFO] Invalid matching multistream found in multistream folder')\n",
    "\n",
    "                # Deleting file from './multistream/compressed' folder\n",
    "                os.remove(os.path.join('./multistream/compressed', filename))\n",
    "\n",
    "                # Downloading file again\n",
    "                download_multistream(file, filename, os.path.join('./multistream/compressed', filename))\n",
    "            else:\n",
    "                print('[INFO] Valid matching multistream found in multistream folder')\n",
    "        else:\n",
    "            # Downloading file\n",
    "            download_multistream(file, filename, os.path.join('./multistream/compressed', filename))\n",
    "\n",
    "        # Extract .bz2 archive\n",
    "        extract_dump(filename)\n",
    "\n",
    "    # Remove .bz2 sufix from archive filename\n",
    "    # (ex) ptwiki-20240720-pages-articles-multistream.xml.bz2 -> ptwiki-20240720-pages-articles-multistream.xml\n",
    "    return filename[:-4]\n",
    "\n",
    "\n",
    "def delete_corrupted_xmls() -> None:\n",
    "    # Delete corrupted/uncompleted extractions\n",
    "    for file in os.listdir('./multistream/compressed'):\n",
    "        if file.endswith('.xml'):\n",
    "            os.remove(os.path.join('./multistream/compressed/', file))\n",
    "\n",
    "\n",
    "def extract_dump(filename: str) -> None:\n",
    "    print('[INFO] Extracting multistream .bz2 file (this might take several minutes)')\n",
    "\n",
    "    # bzip2 -dk ./multistream/compressed/...\n",
    "    # https://superuser.com/a/480951\n",
    "    subprocess.run([\"bzip2\", \"-dk\", './multistream/compressed/' + filename ])\n",
    "    \n",
    "    # https://docs.python.org/3/library/os.html#os.replace\n",
    "    # https://stackoverflow.com/a/8858026\n",
    "    os.replace(os.path.join('./multistream/compressed/', filename[:-4]), os.path.join('./multistream/decompressed/', filename[:-4]))\n",
    "\n",
    "\n",
    "def select_dump() -> Tuple[str, str]:\n",
    "    # Create required directories\n",
    "    create_multistream_dir()\n",
    "\n",
    "    # Delete uncompleted extractions\n",
    "    delete_corrupted_xmls()\n",
    "\n",
    "    # Request user's input for a wikiname/date reference\n",
    "    wikinamedate = input('Fill with wikiname/date [ex: \\'ptwiki/20240720\\']')\n",
    "    print()\n",
    "\n",
    "    # Find, download, check and extract .xml file\n",
    "    filename = get_multistream_file(wikinamedate)\n",
    "\n",
    "    print('[INFO] \\'' + wikinamedate + '\\' selected')\n",
    "\n",
    "    return wikinamedate, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473a6e1c-46ac-4f1c-b80d-6d46107b8e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Fill with wikiname/date [ex: 'ptwiki/20240720'] ptwiki/20240720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Fetching dump info\n",
      "[INFO] Valid matching multistream found in multistream folder\n",
      "[INFO] 'ptwiki/20240720' selected\n",
      "Stored 'filename' (str)\n",
      "Stored 'wikinamedate' (str)\n",
      "CPU times: user 138 ms, sys: 28.3 ms, total: 166 ms\n",
      "Wall time: 4.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find, download, check and extract .xml file\n",
    "wikinamedate, filename = select_dump()\n",
    "\n",
    "# Save variable between different Jupyter notebooks\n",
    "%store filename wikinamedate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
