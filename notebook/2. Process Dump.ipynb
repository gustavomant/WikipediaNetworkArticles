{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c88add-fda4-436b-893d-5c0c18f02ec6",
   "metadata": {},
   "source": [
    "## Process dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb1edaf-bac0-4823-8cd6-20667229bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (13.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: pyre2 in /opt/conda/lib/python3.11/site-packages (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.11/site-packages (from pyarrow) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow pandas tqdm pyre2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548a8f2a-1940-4b40-9eb3-c3bc45a317f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sys import stdout\n",
    "from subprocess import check_output\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "try:\n",
    "    import re2 as re\n",
    "except ImportError:\n",
    "    import re\n",
    "    print('[WARN] Not using re2')\n",
    "\n",
    "\n",
    "\n",
    "def count_pages(filename: str) -> int:\n",
    "    print(f'[INFO] Counting how many \"<pages>\" in \\'{filename}\\'')\n",
    "    command = ['grep', '-wc', '<page>', join('./multistream/decompressed/', filename)]\n",
    "    output = check_output(command).decode(stdout.encoding).strip()\n",
    "\n",
    "    del command\n",
    "\n",
    "    return int(output)\n",
    "\n",
    "\n",
    "def revision(text: str):\n",
    "    if text:\n",
    "        # findall(r'\\[\\[(.*?)\\]\\]')                            > find all [[tag]]\n",
    "        # sub(r'<ref>\\[\\[')                                    > remove <ref>([[tag]])\n",
    "        # sub(r'<quot>\\[\\[')                                   > remove <quot>([[tag]])\n",
    "        # sub(r'\\[\\[([^\\[\\]\\n]*?\\[\\[.*?\\]\\][^\\[\\[\\n]*?)+\\]\\]') > remove [[tag [[tag]]]]    \n",
    "        # sub(r'\\(.*?\\)')                                      > remove ([[tag]])\n",
    "        \n",
    "        while True:\n",
    "            i = start = text.find('[[')\n",
    "            while i != -1:\n",
    "                if text[i + 2:].find('[[') < text[i + 2:].find(']]'):\n",
    "                    i = text[i + 2:].find(']]') + i + 2\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if start == i:\n",
    "                break\n",
    "            else:\n",
    "                text = text[:start] + text[text[i + 2:].find(']]') + i + 4:]\n",
    "\n",
    "        return re.findall(r'(?<!>)\\[\\[(.*?)\\]\\]', \\\n",
    "                          re.sub(r'{{.*?}}', '', \\\n",
    "                                 re.sub(r'\\(.*?\\)', '', text)))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def index_pages(filename: str, wikinamedate: str) -> None:\n",
    "    total_pages = 2_632_633 #count_pages(filename)\n",
    "    context = iter(ET.iterparse(join('./multistream/decompressed/', filename), events=('end',)))\n",
    "    # Initialize variables\n",
    "    chunk_size = 100_000  # Define the size of each chunk\n",
    "    rows = []\n",
    "    pqwriter = None\n",
    "\n",
    "    # Create dir recursively\n",
    "    # https://docs.python.org/3/library/os.html#os.makedirs\n",
    "    os.makedirs(join('./output/', wikinamedate.replace('/', '-')), exist_ok=True)\n",
    "    \n",
    "    with tqdm(total=total_pages, unit=' pages', unit_scale=True, desc='[INFO] Processing pages', initial=0, file=stdout) as pbar:\n",
    "        title, id, namespace = [None] * 3\n",
    "        \n",
    "        for event, elem in context:\n",
    "            match elem.tag:\n",
    "                case '{http://www.mediawiki.org/xml/export-0.11/}title':\n",
    "                    title = elem.text\n",
    "                case '{http://www.mediawiki.org/xml/export-0.11/}ns':\n",
    "                    namespace = elem.text\n",
    "                case '{http://www.mediawiki.org/xml/export-0.11/}id':\n",
    "                    if id == None:\n",
    "                        id = elem.text\n",
    "                case '{http://www.mediawiki.org/xml/export-0.11/}text':\n",
    "                    rows.append([title, id, namespace, revision(elem.text)])\n",
    "                    id = None\n",
    "                    \n",
    "                    pbar.update()\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            if len(rows) >= chunk_size:\n",
    "                df_chunk = pd.DataFrame(rows, columns=['Page Title', 'Page ID', 'Page Namespace', 'Page References'])\n",
    "                table = pa.Table.from_pandas(df_chunk)\n",
    "                \n",
    "                # Append to Parquet file\n",
    "                if pqwriter == None:\n",
    "                    pqwriter = pq.ParquetWriter(join('./output/', wikinamedate.replace('/', '-'), 'raw.parquet'), table.schema) \n",
    "                    \n",
    "                pqwriter.write_table(table)\n",
    "                \n",
    "                rows = []  # Clear rows to free memory\n",
    "\n",
    "\n",
    "    # Save remaining rows if any\n",
    "    if rows:\n",
    "        df_chunk = pd.DataFrame(rows, columns=['Page Title', 'Page ID', 'Page Namespace', 'Page References'])\n",
    "        table = pa.Table.from_pandas(df_chunk)\n",
    "    \n",
    "        if pqwriter == None:\n",
    "            pqwriter = pq.ParquetWriter(join('./output/', wikinamedate.replace('/', '-'), 'raw.parquet'), table.schema) \n",
    "\n",
    "        pqwriter.write_table(table)\n",
    "\n",
    "    if pqwriter:\n",
    "        pqwriter.close()\n",
    "\n",
    "    del title, id, namespace, event, elem, context, rows, pqwriter, chunk_size, df_chunk, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9feb00-9b49-417f-9886-bf260eea87cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing pages: 100%|██████████| 2.63M/2.63M [19:04<00:00, 2.30k pages/s]\n",
      "CPU times: user 18min 48s, sys: 23.5 s, total: 19min 12s\n",
      "Wall time: 19min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Restore variable from different Jupyter notebook\n",
    "%store -r filename wikinamedate\n",
    "\n",
    "index_pages(filename, wikinamedate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
