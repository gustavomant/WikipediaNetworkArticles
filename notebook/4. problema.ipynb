{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3968392b",
   "metadata": {},
   "source": [
    "# Wikipedia Page Reference Analysis\n",
    "\n",
    "This Jupyter notebook contains Python code for analyzing the reference structure of Wikipedia pages. It includes two main components:\n",
    "\n",
    "1. A `Counter` class for processing and analyzing page references\n",
    "2. Interactive cells for user input and result display\n",
    "\n",
    "## Features:\n",
    "\n",
    "- Efficient data handling using Pandas and Parquet\n",
    "- Analysis of reference degrees (how many steps away pages are from a given page)\n",
    "- Calculation of weighted mean distance, total nodes, and network diameter\n",
    "- Option to consider only the first reference or all references for each page\n",
    "\n",
    "## Dependencies:\n",
    "\n",
    "- pandas\n",
    "- os\n",
    "- typing\n",
    "\n",
    "## Note:\n",
    "\n",
    "The analysis can be performed considering either only the first reference of each page or all references. This allows for different perspectives on the Wikipedia link structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f579ebee-eda4-45b5-ae05-e7c4fc4c22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "from igraph import Graph\n",
    "\n",
    "from typing import List, Set  # For type hinting\n",
    "\n",
    "\n",
    "def get_ordinal_suffix(n: int) -> str:\n",
    "    \"\"\"\n",
    "    Get the ordinal suffix for a given integer.\n",
    "\n",
    "    Args:\n",
    "        n (int): The integer for which to determine the ordinal suffix.\n",
    "\n",
    "    Returns:\n",
    "        str: The ordinal suffix (e.g., 'st', 'nd', 'rd', 'th').\n",
    "    \"\"\"\n",
    "\n",
    "    if 10 <= n % 100 <= 20:\n",
    "        return \"th\"\n",
    "    return {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\")\n",
    "\n",
    "\n",
    "def filter_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"[INFO] Applying modifiers to DataFrame\")\n",
    "\n",
    "    # Convert 'Page Title' to lowercase and ensure 'Page References' are lists of lowercase references\n",
    "    df[\"Page Title\"] = df[\"Page Title\"].str.lower()\n",
    "    df[\"Page References\"] = df[\"Page References\"].apply(\n",
    "        lambda refs: [ref.lower() for ref in refs]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_parquet(wikinamedate: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads a processed Parquet file containing Wikipedia page data and returns it as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        wikinamedate (str): The date and name string used to locate the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the processed page data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"[INFO] Reading '{wikinamedate.replace('/', '-')}/processed.parquet'\")\n",
    "\n",
    "    df = pd.read_parquet(\n",
    "        join(\"../output/\", wikinamedate.replace(\"/\", \"-\"), \"processed.parquet\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_graph(df: pd.DataFrame, first_ref: bool, batch_size: int = 10_000) -> Graph:\n",
    "    # Create the directed graph using iGraph\n",
    "    g = Graph(directed=True)\n",
    "\n",
    "    # Extract the page titles and references\n",
    "    page_titles = df['Page Title'].values\n",
    "    references = df['Page References'].values\n",
    "\n",
    "    # Add vertices to the graph\n",
    "    g.add_vertices(page_titles)\n",
    "\n",
    "    # Prepare edges for batch processing\n",
    "    batch_edges = []\n",
    "    \n",
    "    # Iterate through page titles and references using numpy's zip for speed\n",
    "    for page, refs in zip(page_titles, references):\n",
    "        if len(refs) > 0:\n",
    "            if first_ref:\n",
    "                batch_edges.append((page, refs[0]))\n",
    "            else:\n",
    "                batch_edges.extend((page, ref) for ref in refs)\n",
    "\n",
    "        # Add edges in batches for memory and performance efficiency\n",
    "        if len(batch_edges) >= batch_size:\n",
    "            g.add_edges(batch_edges)\n",
    "            batch_edges.clear()  # Efficiently reset the list\n",
    "\n",
    "    # Add any remaining edges to the graph\n",
    "    if batch_edges:\n",
    "        g.add_edges(batch_edges)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def calculate_degree_series(g: Graph, page_title: str, first_ref=False):\n",
    "    # Start with the initial page\n",
    "    starting_vertex = page_title.lower()\n",
    "\n",
    "    # Initialize processed sets and counters\n",
    "    processed_pages = set()\n",
    "    pages_to_process = {starting_vertex}\n",
    "    counts = []\n",
    "\n",
    "    # Breadth-First Search (BFS) to determine the distance from the starting page\n",
    "    while pages_to_process:\n",
    "        degree = len(counts) + 1\n",
    "        print(f\"Checking {degree}{get_ordinal_suffix(degree)} degree of distance\", end=\"\\r\")\n",
    "\n",
    "        # Find the vertices that are at the current degree of reference\n",
    "        new_pages = set()\n",
    "        for v in pages_to_process:\n",
    "            neighbors = g.neighbors(v, mode=\"out\")  # Get all outgoing neighbors\n",
    "            new_pages.update(g.vs[neighbors][\"name\"])\n",
    "\n",
    "        # Calculate the new pages to process and update counts\n",
    "        new_pages -= processed_pages  # Remove already processed pages\n",
    "        counts.append(len(new_pages))\n",
    "        processed_pages.update(new_pages)\n",
    "        pages_to_process = new_pages\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(f\"Degree Series: {str(counts)}\")\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f180bc0d-677d-4dcf-93c6-30adaaf7e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading 'ptwiki-20240720/processed.parquet'\n",
      "[INFO] Applying modifiers to DataFrame\n",
      "\n",
      "CPU times: user 50.7 s, sys: 4.58 s, total: 55.3 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Restore variable from different Jupyter notebook\n",
    "%store -r wikinamedate\n",
    "\n",
    "# Reads a processed Parquet file containing Wikipedia page data\n",
    "df = filter_df(read_parquet(wikinamedate))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acbca10b-6a82-4be0-8ec9-ce4adb6f70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 52.9 s, sys: 149 ms, total: 53 s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "first_ref = True\n",
    "\n",
    "# Create Graph\n",
    "graph = create_graph(df, True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cad775b-1393-406c-a899-c4985486a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<igraph.seq.EdgeSeq at 0x7f67462b9550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
